{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 2: Linear Discriminant Functions and Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The MNIST database (Modified National Institute of Standards and Technology Database) is a large collection of handwritten digits which happens to be a state of the art data set to train and test machine learning models using image processing.It consist of 60000 Training samples and 10000 Testing Samples. Each sample image consist of normalized $28X28$ pixels grayscale image stored as 784 dimensional vector for each sample.This is a way of dimension reduction done by the MNIST to use it for model developments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the datasets\n",
    "For loading the given MNIST datasets we will be using the mnist library to fetch the data.The MNIST() function will be used to fetch the datasets from the directory './data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from mnist.loader import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "m = MNIST('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the different datasets as the X-Training and Y-Training set which has 60000 samples which will be used to train the classification model and then the X-Test and Y-test data set which has 10000 samples will be used to test the model. After seperating the datasets , the data sets will be converted to ND array and as float values which will be benificial for the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,ytrain = m.load_training()\n",
    "xtest,ytest = m.load_testing()\n",
    "xtrain = np.asarray(xtrain).astype(np.float32)\n",
    "ytrain = np.asarray(ytrain).astype(np.float32)\n",
    "xtest = np.asarray(xtest).astype(np.float32)\n",
    "ytest = np.asarray(ytest).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 :Train the Multi Class Support Vector Classifier with 1-Norm and Dot Product Kernel\n",
    "For training the linear SVM an L1 norm will be used which is the Hinge loss.The training data is normalized before training it to decrease the convergence time and increase accuracy. The SVC Classification minimization function is given as \n",
    "\\begin{equation}\n",
    "C\\sum_{i=1,n}L(f(x_i),y_i)+\\Omega(w)\n",
    "\\end{equation}\n",
    "Where $C$ is the amount which sets the impact of the regularization. This will be set by the process of cross validation. We will be using the LinearSVC function to fit in the training data to build the SVC Classifier and the StandardScaler function will be used to normalize the input data during training and prediction using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moham\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='hinge', max_iter=2000, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scl = StandardScaler()\n",
    "scl.fit(xtrain)\n",
    "svc = LinearSVC(loss = 'hinge',C = 1,max_iter = 2000)\n",
    "# svc.fit(xtrain, ytrain)\n",
    "svc.fit(scl.transform(xtrain), ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through cross validation the $C$ parameter is found to be $1$ when a maximum iteration of 2000 is set to during building the model.We will predict the target values using the SVC Classifier built with the testing sample dataset and then compute the accuracy and error with respect to the true target values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy -  91.82000000000001 %\n",
      "Error -  29.79 %\n"
     ]
    }
   ],
   "source": [
    "ypred = np.zeros(ytest.shape)\n",
    "xtest_n = scl.transform(xtest)\n",
    "for i in range(0,ypred.shape[0]):\n",
    "    x = np.array([xtest_n[i]])\n",
    "    ypred[i] = svc.predict(x)\n",
    "true = 0\n",
    "error = 0\n",
    "for i in range(0,ytest.shape[0]):\n",
    "    error = error + ((ytest[i]-ypred[i])**2)**0.5\n",
    "    if(ytest[i] == ypred[i]):\n",
    "        true = true+1\n",
    "        \n",
    "accuracy = true/ypred.shape[0]\n",
    "Error = error/ypred.shape[0] \n",
    "\n",
    "print('Accuracy - ',accuracy*100,'%')\n",
    "print('Error - ',Error*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 :Lagrange dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Given Problem is\n",
    "$Features$ $(x_1,y_1),....(x_N,y_N)$ $where y_1,...y_N\\in{-1,1}$ which is also the primal problem is given as \n",
    "\\begin{equation}\n",
    "Minimize \\; w^T.w+C\\sum_{i=1}^N\\xi_i \\; where \n",
    "\\\\\n",
    "Subject\\;to\\;y_i(w^T.x_i)\\geq1-\\xi_i\n",
    "\\\\\n",
    "and\\;\\;\\xi_i\\geq0 \n",
    "\\end{equation}\n",
    "The inequality constraints can be expressed as \n",
    "\\begin{equation}\n",
    "2-y_i(w^T.x_i)-\\xi_i\\leq0\n",
    "\\\\\n",
    "1-\\xi\\leq0\n",
    "\\end{equation}\n",
    "To solve it using the Dual Method we will be converting it to Lagrange form.\n",
    "The Lagrange form of the problem is given as \n",
    "\\begin{equation}\n",
    "L(w,\\alpha) = w^T.w+C\\sum_{i=1}^N\\xi_i + \\alpha_1(2-y_i(w^T.x_i)-\\xi_i)+\\alpha_2(1-\\xi)\n",
    "\\end{equation}\n",
    "where $\\alpha_1$ & $\\alpha_2$ are the Lagrange Multipliers or the KKT multipliers where $\\alpha_i\\geq0$.\n",
    "Set the partial derivative function to Zero.\n",
    "\\begin{equation}\n",
    "\\partial_wL = 2w - \\alpha_1y_ix_i= 0\n",
    "\\\\\n",
    "w = \\frac{\\alpha_1y_ix_i}{2}\n",
    "\\end{equation}\n",
    "The marigin is given as\n",
    "\\begin{equation}\n",
    "\\gamma = y\\frac{w^Tw+b}{||w||}\n",
    "\\end{equation}\n",
    "In SVM the a criterion is defined by a decision surface that maximally far away from any data point and the distance between the decision surface to the closest data point is called the marigin.The marigin needs to be maximized because the points nearer to the decision surface has high uncertainity with the decision function which classifies the data point where as a large marigin gives low uncertain classifications.This allows to handle small miscalculations and errors in the data sets. \n",
    "Solving Dual problem is beneficial over primal problem because in Primal problem we only obtain the optimal $w$ where as in the Dual problem we take $\\alpha_i$ in consideration.The $\\alpha_i$ is useful because it decides the importance of the subjected features and computation of the classification can be faster than computing the scalar product $w^Tx$ directly in the case of the primal problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
